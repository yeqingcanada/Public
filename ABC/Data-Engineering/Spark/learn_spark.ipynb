{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "# https://www.youtube.com/watch?v=_C8kWso4ne4&t=3524s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab24c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pyspark=spark.read.csv('test1.csv')\n",
    "df_pyspark=spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "# 以下结果，不包含表头\n",
    "df_pyspark.show()\n",
    "# 包含表头\n",
    "df_pyspark=spark.read.option('header','true').csv('test1.csv')\n",
    "df_pyspark.show()\n",
    "# show the head 3 rows\n",
    "df_pyspark.head(3)\n",
    "# show column info（名字，数据类型，是否可为null）\n",
    "df_pyspark.printSchema()\n",
    "# 加了inferSchema true后，会自动锁定数据结构，默认都是string，加了后整数会是integer\n",
    "df_pyspark=spark.read.option('header','true').csv('test1.csv', inferSchema=True)\n",
    "# 上面一行的另一种写法\n",
    "df_pyspark=spark.read.csv('test1.csv',header=True,inferSchema=True)\n",
    "df_pyspark.printSchema()\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 查看column名字\n",
    "df_pyspark.columns\n",
    "# 查看一整列column，利用select\n",
    "df_pyspark.select(['Name','Experience']).show()\n",
    "# 查看每一列的名字和数据类型\n",
    "df_pyspark.dtypes\n",
    "# 类似pandas，count,mean,方差，min，max\n",
    "df_pyspark.describe().show()\n",
    "# ---------------------------------------------------------------------------------\n",
    "# adding columns\n",
    "df_pyspark=df_pyspark.withColumn('Experience After 2 year',df_pyspark['Experience']+2)\n",
    "df_pyspark.show()\n",
    "# drop columns\n",
    "df_pyspark=df_pyspark.drop('Experience After 2 year')\n",
    "# rename column\n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4711338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pyspark=spark.read.csv('test2.csv',header=True)\n",
    "df_pyspark=spark.read.csv('test2.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "\n",
    "# drop na rows,只要一行包含一个na，改行即被删除\n",
    "df_pyspark.na.drop().show()\n",
    "# 至少有3个非空的cell的行才会被留下\n",
    "df_pyspark.na.drop(how=\"any\",thresh=3).show()\n",
    "# 去掉age这列null的行\n",
    "df_pyspark.na.drop(how=\"any\",subset=['Age']).show()\n",
    "# Filling the Missing Value，如果inferSchema=True，下面这句只会填string的列，integer的列依然null\n",
    "df_pyspark.na.fill('Missing Values',['Experience','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46468e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# 如果是空，就用平均值/中位数代替\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "    ).setStrategy(\"median\")\n",
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1452a",
   "metadata": {},
   "source": [
    "### Filter Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ffceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -l                               # 列出所有themes\n",
    "!jt -t oceans16                      # 海洋主题好丑\n",
    "!jt -t chesterish -T -N -kl          # 后面三个参数是为了显示toolbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ac36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter operation\n",
    "# data of from test1.csv\n",
    "\n",
    "# 具体操作参考Tutorial 4- Pyspark Dataframes- Filter operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa7176",
   "metadata": {},
   "source": [
    "### Pyspark GroupBy And Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  data is from test3.csv\n",
    "df_pyspark=spark.read.csv('test3.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()\n",
    "df_pyspark.printSchema()\n",
    "\n",
    "# 具体操作参考Tutorial 5- Pyspark With Python-GroupBy And Aggregate Functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
